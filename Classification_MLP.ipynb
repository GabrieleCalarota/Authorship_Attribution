{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec average as sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function to clean the text\n",
    "def cleanSentence(sentence):\n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    return words\n",
    "\n",
    "#Define another function that will take the average word2vec of sentence\n",
    "def averageSentence(words):\n",
    "    average_vector = np.zeros((1,300))\n",
    "    for word in words: \n",
    "        #Check if the word is in the previously loaded model\n",
    "        if word in model.vocab:\n",
    "            average_vector += model[word]\n",
    "        #If it's not in the model then just place them as zero vector\n",
    "        else:\n",
    "            average_vector += np.zeros((1,300))\n",
    "    #Divide by te overall number of words in a sentence\n",
    "    resulting_vector = average_vector / len(words)\n",
    "    return resulting_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"I hate this movie. It is horrible.\"\n",
    "sentence2 = \"Great movie and amazing actors.\"\n",
    "#Now we can use the two function above to have an average vector representation\n",
    "words1 = cleanSentence(sentence1)    \n",
    "resulting_vector1 = averageSentence(words1)\n",
    "#For the second one\n",
    "words2 = cleanSentence(sentence2)    \n",
    "resulting_vector2 = averageSentence(words2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data points\n",
    "resulting_vector1 = resulting_vector1.tolist() \n",
    "resulting_vector2 = resulting_vector2.tolist()\n",
    "\n",
    "#Take the first half as 1st and 2nd half as the 2nd data point for each data\n",
    "tmp1 = [sum(resulting_vector1[0][0:int(len(resulting_vector1[0])/2)])/int(len(resulting_vector1[0])/2),\n",
    "        sum(resulting_vector1[0][int(len(resulting_vector1[0])/2):int(len(resulting_vector1[0]))])/int(len(resulting_vector1[0])/2)]\n",
    "\n",
    "tmp2 = [sum(resulting_vector2[0][0:int(len(resulting_vector2[0])/2)])/int(len(resulting_vector2[0])/2),\n",
    "        sum(resulting_vector2[0][int(len(resulting_vector2[0])/2):int(len(resulting_vector2[0]))])/int(len(resulting_vector2[0])/2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [tmp1, tmp2]\n",
    "\n",
    "#Label the first sentence as the negative one and the second one as the positive one\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X, y)                         \n",
    "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "       epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate this movie. It is horrible.\n",
      "Great movie and amazing actors.\n",
      "[[0.0019474976403372573, 0.000868452276502335], [-0.000700768152872721, -0.008509938557942711]]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrible and useless movie.\n",
      "Fantastic. I love it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3 = \"Terrible and useless movie.\"\n",
    "sentence4 = \"Fantastic. I love it.\"\n",
    "\n",
    "#We need to do the same data manuplation techniques as before\n",
    "words3 = cleanSentence(sentence3)    \n",
    "resulting_vector3 = averageSentence(words3)\n",
    "#For the second one\n",
    "words4 = cleanSentence(sentence4)    \n",
    "resulting_vector4 = averageSentence(words4)\n",
    "\n",
    "#Create data points\n",
    "resulting_vector3 = resulting_vector3.tolist()\n",
    "resulting_vector4 = resulting_vector4.tolist()\n",
    "\n",
    "#Take the first half as 1st and 2nd half as the 2nd data point for each data\n",
    "tmp3 = [sum(resulting_vector3[0][0:int(len(resulting_vector3[0])/2)])/int(len(resulting_vector3[0])/2),\n",
    "        sum(resulting_vector3[0][int(len(resulting_vector3[0])/2):int(len(resulting_vector3[0]))])/int(len(resulting_vector3[0])/2)]\n",
    "\n",
    "tmp4 = [sum(resulting_vector4[0][0:int(len(resulting_vector4[0])/2)])/int(len(resulting_vector4[0])/2),\n",
    "        sum(resulting_vector4[0][int(len(resulting_vector4[0])/2):int(len(resulting_vector4[0]))])/int(len(resulting_vector4[0])/2)]\n",
    "\n",
    "print(sentence3)\n",
    "print(sentence4)\n",
    "#Make predictions\n",
    "clf.predict([tmp3, tmp4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
